{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sukyoung11/mobis/blob/main/DL_3_A_NN_backpropagation_ipynb%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8g0GPn9R4Tqq"
      },
      "outputs": [],
      "source": [
        "# Importing 'numpy' library\n",
        "import numpy as np\n",
        "# Importing 'matplotlib' library to plot experimental results in form of figures\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3층 신경망(입력층-은닉층-출력층) 클래스 정의\n",
        "class ThreeLayersNeuralNetwork():\n",
        "    def __init__(self):\n",
        "        # 난수 시드 고정: 실행할 때마다 동일한 난수가 생성되어 디버깅에 유리\n",
        "        np.random.seed(1)\n",
        "\n",
        "       # 3층 신경망 구조:\n",
        "        # - 입력층(Layer 0): 특성 3개\n",
        "        # - 은닉층(Layer 1): 뉴런 4개\n",
        "        # - 출력층(Layer 2): 뉴런 1개\n",
        "\n",
        "        # 입력층(3) -> 은닉층(4) 가중치 초기화\n",
        "        # 가중치 값의 범위: [-1, 1]\n",
        "        # 형태: (3 x 4) = 입력 3개 × 은닉 4개\n",
        "        self.weights_0_1 = 2 * np.random.random((3, 4)) - 1\n",
        "\n",
        "        # 은닉층(4) -> 출력층(1) 가중치 초기화\n",
        "        # 가중치 값의 범위: [-1, 1]\n",
        "        # 형태: (4 x 1) = 은닉 4개 × 출력 1개\n",
        "        self.weights_1_2 = 2 * np.random.random((4, 1)) - 1\n",
        "\n",
        "        # 학습/추론 시 출력층의 값을 보관할 변수(행렬)\n",
        "        self.layer_2 = np.array([])\n",
        "\n",
        "     # 시그모이드 활성화 함수: 값을 (0,1) 범위로 정규화\n",
        "    def normalizing_results(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    # 시그모이드의 도함수: a * (1 - a)  (여기서 a는 시그모이드 출력)\n",
        "    # 역전파 시 가중치 보정 크기(기울기)를 계산할 때 사용\n",
        "    def derivative_of_sigmoid(self, x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "    # 학습 후/중 추론(순전파) 실행 함수\n",
        "    def run_nn(self, set_of_inputs):\n",
        "        # 순전파: 입력 -> 은닉 -> 출력\n",
        "        # 각 층에서 선형결합(dot) 후 시그모이드로 비선형 변환\n",
        "        layer_0 = set_of_inputs  # matrix 1x3\n",
        "        layer_1 = self.normalizing_results(np.dot(layer_0, self.weights_0_1))  # matrix 1x3 * matrix 3x4 = matrix 1x4\n",
        "        layer_2 = self.normalizing_results(np.dot(layer_1, self.weights_1_2))  # matrix 1x4 * matrix 4x1 = matrix 1x1\n",
        "        return layer_2\n",
        "\n",
        "    # 신경망 학습(역전파 포함)\n",
        "    def training_process(self, set_of_inputs_for_training, set_of_outputs_for_training, iterations):\n",
        "        # 지정된 반복 횟수만큼 에폭 반복\n",
        "        for i in range(iterations):\n",
        "             # 1) 순전파: 입력 -> 은닉 -> 출력\n",
        "            # numpy.dot으로 층의 값과 가중치를 행렬 곱해 선형결합 수행\n",
        "            layer_0 = set_of_inputs_for_training  # matrix 4x3\n",
        "            layer_1 = self.normalizing_results(np.dot(layer_0, self.weights_0_1))  # matrix 4x3 * matrix 3x4 = matrix 4x4\n",
        "            self.layer_2 = self.normalizing_results(np.dot(layer_1, self.weights_1_2))  # matrix 4x4 * matrix 4x1 = matrix 4x1\n",
        "\n",
        "            # 2) 출력층 오차: (정답 - 예측)\n",
        "            # 형태: (4 x 1) - (4 x 1) = (4 x 1)\n",
        "            layer_2_error = set_of_outputs_for_training - self.layer_2\n",
        "\n",
        "            # 500번마다 평균 절대오차를 출력(학습 경향 관찰용)\n",
        "            # (분석 전 출력 과다 방지를 원하면 이 부분을 주석 처리)\n",
        "            if (i % 500) == 0:\n",
        "                print('Final error after', i, 'iterations =', np.mean(np.abs(layer_2_error)))\n",
        "\n",
        "            # 3) 출력층 델타(delta_2) 계산: 오차 × 시그모이드 도함수\n",
        "            # 여기서는 원소별 곱(element-wise)을 사용('*'), dot(행렬곱)이 아님\n",
        "            # 예) [[1],[1],[1],[1]] * [[2],[3],[4],[5]] = [[2],[3],[4],[5]]\n",
        "\n",
        "            delta_2 = layer_2_error * self.derivative_of_sigmoid(self.layer_2)\n",
        "\n",
        "            # 4) 은닉층 오차: delta_2를 출력층 가중치로 역전파\n",
        "            # delta_2 · W(은닉->출력)^T  => (4x1)·(1x4)=(4x4)\n",
        "            # (각 샘플의 은닉 노드별 오차 기여도)\n",
        "            layer_1_error = np.dot(delta_2, self.weights_1_2.T)\n",
        "\n",
        "            # 5) 은닉층 델타(delta_1): 은닉층 오차 × 시그모이드 도함수\n",
        "            delta_1 = layer_1_error * self.derivative_of_sigmoid(layer_1)\n",
        "\n",
        "            # 6) 가중치 업데이트(경사상승 형태; 손실을 줄이려면 -grad가 일반적이나\n",
        "            #    여기 구현은 목표-예측 형태와 누적 규칙에 맞춰 += 사용)\n",
        "            # 은닉->출력 가중치: (4x4)^T·(4x1) = (4x1)\n",
        "            self.weights_1_2 += np.dot(layer_1.T, delta_2)\n",
        "            # 입력->은닉 가중치: (4x3)^T·(4x4) = (3x4)\n",
        "            self.weights_0_1 += np.dot(layer_0.T, delta_1)\n"
      ],
      "metadata": {
        "id": "BuVWUBHnH8t-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ThreeLayersNeuralNetwork 클래스를 인스턴스화하여 3층 신경망을 하나 생성합니다.\n",
        "#        __init__ 안에서 np.random.seed(1)로 시드를 고정했기 때문에,\n",
        "#        실행할 때마다 같은 초기 가중치가 만들어집니다(디버깅/재현성 확보).\n",
        "three_layers_neural_network = ThreeLayersNeuralNetwork()\n",
        "\n",
        "#   초기(학습 전) 가중치를 확인합니다.\n",
        "#        아래는 입력층(Layer 0) → 은닉층(Layer 1) 사이의 가중치 행렬을 출력.\n",
        "#        모양(shape)은 (3 x 4): 입력 특성 3개 × 은닉 뉴런 4개.\n",
        "#        각 값은 ∈ [-1, 1] 범위의 난수로 초기화.\n",
        "print('Weights 0-1')\n",
        "print(three_layers_neural_network.weights_0_1)\n",
        "print()\n",
        "\n",
        "#        은닉층(Layer 1) → 출력층(Layer 2) 사이의 가중치 행렬을 출력.\n",
        "#        모양(shape)은 (4 x 1): 은닉 뉴런 4개 × 출력 뉴런 1개.\n",
        "#        역시 초기 상태이므로 학습 전 무작위 값.\n",
        "print('Weights 1-2')\n",
        "print(three_layers_neural_network.weights_1_2)\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6IFpGSIH_Wu",
        "outputId": "8307c084-731a-4ce7-d01c-1c729d709d44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights 0-1\n",
            "[[-0.16595599  0.44064899 -0.99977125 -0.39533485]\n",
            " [-0.70648822 -0.81532281 -0.62747958 -0.30887855]\n",
            " [-0.20646505  0.07763347 -0.16161097  0.370439  ]]\n",
            "\n",
            "Weights 1-2\n",
            "[[-0.5910955 ]\n",
            " [ 0.75623487]\n",
            " [-0.94522481]\n",
            " [ 0.34093502]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  학습에 사용할 입력/정답(타겟) 데이터를 생성.\n",
        "#  여기서는 총 4개의 샘플을 사용하며, 각 입력은 3개의 특성(feature)로 구성.\n",
        "#  마지막 열이 모두 1인 이유: 이 구현에는 별도 바이어스 항이 없기 때문에,\n",
        "#  \"항상 1인 입력 특성\"을 추가하여 바이어스 역할을 하게 합니다(일명 bias trick).\n",
        "input_set_for_training = np.array([[1, 1, 1], [1, 0, 1], [0, 0, 1], [0, 1, 1]])\n",
        "\n",
        "# 정답(타겟) 벡터 생성 : np.array([[1, 1, 0, 0]])\n",
        "# 모양은 (1 x 4)이므로, .T(전치)를 적용해 (4 x 1) 열벡터로 바꿈.\n",
        "# 즉, 4개 샘플 각각의 타겟이 위에서 아래로 정렬된 형태가 됩니다.\n",
        "output_set_for_training = np.array([[1, 1, 0, 0]]).T\n",
        "\n",
        "# 신경망 학습:\n",
        "# 마지막 인자 5000은 반복 횟수(iterations)로, 순전파/역전파를 5000번 수행합니다.\n",
        "# 내부적으로는:\n",
        "#   1) 순전파: layer_0 → layer_1 → layer_2\n",
        "#   2) 오차 계산: (타겟 - 예측)\n",
        "#   3) 역전파: delta 계산 후, 가중치(weights_0_1, weights_1_2) 갱신\n",
        "#  을 반복하여 출력이 타겟에 근접하도록 가중치를 조정.\n",
        "three_layers_neural_network.training_process(input_set_for_training, output_set_for_training, 5000)\n",
        "\n",
        "# 학습이 끝난 뒤, 최종 출력(layer_2)을 확인합니다.\n",
        "# layer_2는 시그모이드 출력을 사용하므로 (0,1) 범위의 실수이며,\n",
        "# 이진 분류처럼 사용하려면 보통 0.5 기준으로 0/1 판정을 합니다.\n",
        "\n",
        "print()\n",
        "print('Output results after training:')\n",
        "print(three_layers_neural_network.layer_2)\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gc5m07S6IB5Y",
        "outputId": "917d43b8-015c-42fe-b485-a78a7db765ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final error after 0 iterations = 0.4685343254580603\n",
            "Final error after 500 iterations = 0.02735966511749842\n",
            "Final error after 1000 iterations = 0.018014239352682856\n",
            "Final error after 1500 iterations = 0.014245380154921867\n",
            "Final error after 2000 iterations = 0.012098118827883314\n",
            "Final error after 2500 iterations = 0.010673901313210884\n",
            "Final error after 3000 iterations = 0.009643630560125675\n",
            "Final error after 3500 iterations = 0.008855140776037978\n",
            "Final error after 4000 iterations = 0.008227317734746435\n",
            "Final error after 4500 iterations = 0.007712523196874705\n",
            "\n",
            "Output results after training:\n",
            "[[0.99181271]\n",
            " [0.9926379 ]\n",
            " [0.00744159]\n",
            " [0.00613513]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단일 테스트 입력에 대한 신경망의 예측값을 출력합니다.\n",
        "#  np.array([1, 0, 0])은 길이 3의 입력 벡터입니다.\n",
        "#  이 구현에서는 학습 시 마지막 특성(세 번째 값)을 'bias=1'로 두는 bias trick을 썼는데,\n",
        "#  여기서는 0을 넣었기 때문에 학습 때와 조건이 달라집니다(출력이 달라질 수 있음).\n",
        "#  학습 조건과 동일하게 시험하려면 보통 np.array([1, 0, 1])처럼 마지막 값을 1로 둡니다.\n",
        "#   run_nn(...)은 순전파(입력→은닉→출력)를 수행해 시그모이드(0~1) 범위의 값을 반환합니다.\n",
        "#   이 값은 확률처럼 해석할 수 있으며, 이진 분류에선 통상 0.5를 기준으로 0/1 판정을 합니다.\n",
        "#\n",
        "print('Output result for testing data = ', three_layers_neural_network.run_nn(np.array([1, 0, 0])))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGrrBN9jIEiO",
        "outputId": "01d5f73a-cc58-4909-b868-0cada967f6e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output result for testing data =  [0.99619533]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k_y0NfWOHd6d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}